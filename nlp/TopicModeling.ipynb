{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing and topic modeling with Gensim + NLTK\n",
    "\n",
    "This notebook provides a brief introduction to the [Natural Language ToolKit (NLTK)](https://www.nltk.org/) and [gensim](https://radimrehurek.com/gensim/index.html).\n",
    "\n",
    "NLTK has tons of features, the vast majority of which we won't use. Perhaps the most useful aspect is that it ships with a number of text corpora on which to train. In addition, it has modules for processing text (tokenization, stemming/lemmatization), parsing sentences, tagging parts of speech, analyzing word sentiment (SentiWordNet), spelling correction, and much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "from nltk.stem import lancaster\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to grab the \"treebank\" corpus for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is particular about how it logs; just run the below to make things print nicely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Discover a reasonable topic distribution on the treebank corpus. (Using LSI, LDA, whatever.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick peek at the first several sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, s in enumerate(treebank.sents()):\n",
    "    if i > 5:\n",
    "        # stop printing after the first 5\n",
    "        break\n",
    "    print(' '.join(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Our data is already split up into tokens, but you might need to tokenize in your applications. NLTK to the rescue:\n",
    "* `nltk.tokenize.sent_tokenize()` for breaking into sentences\n",
    "* `nltk.tokenize.word_tokenize()` for breaking into words\n",
    "* `nltk.tokenize.casual.casual_tokenize()` for Twitter-aware tokenizing (better punctuation handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"It's been too long since i've seen a @RascalFlatts concert. #countrymusic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.tokenize.word_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.tokenize.casual.casual_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords, stemming\n",
    "\n",
    "There are a lot of filler words that we don't want to play a role in our modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords = stopwords.union({'said', 'would', 'could', 'should', 'may', 'one', 'two', 'may', 'know', 'get', \"i'm\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stemming* reduces words down to a common root. There are a few different stemmers available in NLTK, e.g. `nltk.stem.porter.PorterStemmer`, `nltk.stem.lancaster.LancasterStemmer`, and `nltk.stem.SnowballStemmer`. We'll just use Lancaster here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = lancaster.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('maximum'), stemmer.stem('apple'), stemmer.stem('establishment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords, punctuation, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sentence(s):\n",
    "    r = [tok.lower() for tok in s]\n",
    "    r = [tok for tok in r if re.match(r'[a-z0-9][a-z0-9\\.\\']+', tok)]\n",
    "    return [stemmer.stem(t) for t in r if t not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Never do this in production! But for our purposes, we want to be able to have some insight into the raw documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = [s for s in treebank.sents()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we remove lowercase, remove stopwords, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [normalize_sentence(s) for s in raw_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert our corpus into bag-of-words now, with the aim of converting to TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = collections.Counter()\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        word_counts[token] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of tokens that appear only once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    [token for token in text if word_counts[token] > 1]\n",
    "    for text in texts\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use some gensim magic to convert to bag-of-words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lexicon = corpora.Dictionary(texts)\n",
    "corpus = [corpus_lexicon.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what we have is a bag-of-words model for each document in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 110\n",
    "print(' '.join(raw_texts[n]))\n",
    "print(texts[n])\n",
    "print(corpus[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll convert to TF-IDF (term frequency-inverse document frequency):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus, normalize=True)\n",
    "tfidf_corpus = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try LSI (latent semantic indexing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model = models.LsiModel(tfidf_corpus, id2word=corpus_lexicon, num_topics=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the topics learned, use `show_topics`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or just look at a single topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model.show_topic(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try LDA now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = models.LdaModel(corpus, id2word=corpus_lexicon, num_topics=5, passes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that picking the number of topics is tricky!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.show_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
